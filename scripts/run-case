#!/usr/bin/env python
# Copyright (c) 2020 MesoSim Developers.
# Distributed under the terms of the Apache 2.0 License.
# SPDX-License-Identifier: Apache-2.0
"""
Run a Full Case
---------------

This script does basically everything:

- triggers LSR loading in the frontend API
- archives warnings
- queries Amazon S3 for valid level II files for the time range of the case
- create the staging dirs for radars (K* subdirs with empty dir.list, config.cfg,
  grlevel2.cfg)
- on trigger, start the simulation, which will run a "sleepy loop" that will
    - Munge warning text for current time and append to hourly warning text file
    - Download radar file(s) from S3 to temp dir, unzip them, munge them to current time, copy
      to target dir, and update dir.list

Warning data provided by the IEM's wonderful JSON APIs. Level II data by NOAA's Big Data
Project on Amazon S3.

"""

# Imports
import argparse
import bz2
from datetime import datetime, timedelta
from dateutil import parser as dateparser
import getpass
import os
import re
import shutil
from sqlite3 import dbapi2 as sql
import struct
import subprocess

import boto3
from mesosim.core.timing import arc_time_from_cur, cur_time_from_arc, std_fmt
from mesosim.warning import process_warning_text
import pandas as pd
import pytz
import requests


# Constants
default_sleep = 15
s3 = boto3.client('s3')
l2_datetime_pattern = re.compile(
    r"(?:K[A-Z]{3})(?P<Y>[0-9]{4})(?P<m>[0-9]{2})(?P<d>[0-9]{2})_(?P<H>[0-9]{2})"
    r"(?P<M>[0-9]{2})(?P<S>[0-9]{2})"
)

# Functions
def now():
    return datetime.now(tz=pytz.UTC)


def check_running(api_base):
    r = requests.get(api_base + "simulation/running")
    return r.status_code < 400 and r.json()['running']


def debz(oldfn, newfn):
    """Ported from daryl's l2munger helper script; not sure if needed, but here in case."""
    if os.path.isfile(newfn):
        print("Error: refusing to overwrite existing file '%s'" % (newfn, ))
        return
    output = open(newfn, 'wb')
    fobj = open(oldfn, 'rb')

    output.write(fobj.read(24))
    while True:
        sz = struct.unpack('>L', fobj.read(4))[0]
        chunk = fobj.read(sz)
        if not chunk:
            break
        output.write(bz2.decompress(chunk))
        # unsure of this
        if sz != len(chunk):
            break

    output.close()


if __name__ == '__main__':
    # Set up script configuration
    parser = argparse.ArgumentParser(
        description="Run the chase simulation for an archive case.", usage=argparse.SUPPRESS
    )

    parser.add_argument(
        "-R",
        "--radar-sites",
        nargs="+",
        help="Radar sites (exclude to run without radar)",
        metavar="KLNX LUEX KOAX",
    )
    parser.add_argument(
        "-W",
        "--warning-wfos",
        nargs="+",
        help="Warning WFOs (exclude to run without warnings)",
        metavar="LBF GID OAX",
    )
    parser.add_argument(
        "-L",
        "--lsr-wfos",
        nargs="+",
        help="LSR WFOs (exclude to not trigger LSR loading on api)",
        metavar="LBF GID OAX",
    )
    parser.add_argument(
        "-s",
        "--start",
        help="<Required> Archival start time",
        required=True,
        metavar="2010-06-01T00:00Z",
    )
    parser.add_argument(
        "-e",
        "--end",
        help="<Required> Archival end time",
        required=True,
        metavar="2010-06-01T06:00Z",
    )
    parser.add_argument(
        "-S",
        "--speed-factor",
        help="<Required> Simulation speed-up factor (integer only)",
        required=True,
        metavar="4",
    )
    parser.add_argument(
        "-A",
        "--api-base",
        help="API Base URL",
        metavar="https://chase.iawx.info/api",
        default="https://chase.iawx.info/api"
    )
    parser.add_argument(
        "--warning-db",
        help="Warning DB (will be created if not exists)",
        default="/home/jthielen/warnings.db",
        metavar="/home/jthielen/warnings.db"
    )
    parser.add_argument(
        "-w",
        "--warning-dir",
        help="<Required> Warning (web) directory",
        required=True,
        metavar="/var/www/warnings/"
    )
    parser.add_argument(
        "-r",
        "--radar-dir",
        help="<Required> Radar base (web) directory",
        required=True,
        metavar="/var/www/radar/"
    )
    parser.add_argument(
        "-t",
        "--radar-temp-dir",
        help="Temporary radar staging directory (download, extract, and munge)",
        metavar="/tmp/",
        default="/tmp/"
    )
    parser.add_argument(
        "--reload-state",
        help="Set to 1 if this is a reload of a failed/quit in-process simulation, rather than new",
        type=int,
        default=0,
        metavar="0"
    )
    parser.add_argument(
        "--check-remote",
        help="Set to 1 if we should regularly ping the main API to see if the simulation is still running (must be 0 to run independently",
        type=int,
        default=1,
        metavar="1"
    )

    ####################
    # Parsed Arguments #
    ####################

    try:
        args = parser.parse_args()
    except SystemExit:
        parser.print_help()
        raise

    # Archive Times
    arc_start_time = dateparser.parse(args.start)
    arc_end_time = dateparser.parse(args.end)
    speed_factor = int(args.speed_factor)
    
    # Other
    fresh = not args.reload_state

    # Get Admin Passcode
    admin_passcode = getpass.getpass("Enter API Admin Passcode: ")

    ########################
    # Trigger LSR Archival #
    ########################

    if args.lsr_wfos and fresh:
        print("\n\nTriggering LSR Archival...\n\n")
        
        # Make request to server
        r = requests.post(
            args.api_base + "/placefile/lsr/load",
            data={
                "auth": admin_passcode,
                "start": arc_start_time.strftime("%Y%m%d%H%M"),
                "end": arc_end_time.strftime("%Y%m%d%H%M"),
                "wfos": ",".join(wfos),
            }
        )

        # Dump response
        print(r)
        print(r.text)

        lsr_count = r.json()["count"]
    elif not fresh:
        print("Reloading from past state...skip archival steps...")
    else:
        print("No LSR WFOs provided...skipping...")

    ####################
    # Warning Archival #
    ####################

    if args.warning_wfos and fresh:
        handle_warnings = True
        print("\n\nArchiving warnings...\n\n")

        # Build the endpoint URL to access the IEM archive
        warnings_endpoint = (
            "http://mesonet.agron.iastate.edu/json/"
            + "nwstext_search.py?sts={start}&ets={end}&"
            + "awipsid={hazard}{wfo}"
        )

        # Loop over all the warning products
        warnings = []
        for hazard in ["TOR", "SVR", "FFW"]:
            for wfo in args.warning_wfos:
                print("\tDownloading {}{}...".format(hazard, wfo))
                r = requests.get(
                    warnings_endpoint.format(
                        start=arc_start_time.strftime("%Y-%m-%dT%H:%MZ"),
                        end=arc_end_time.strftime("%Y-%m-%dT%H:%MZ"),
                        hazard=hazard,
                        wfo=wfo,
                    )
                )
                warnings += [record["data"] for record in r.json()["results"]]

        # And let's just add the record close character so we don't have to later
        warnings = [warning + "\x03" for warning in warnings]

        # Make the warning database
        print("\tAccessing local database...")

        warn_con = sql.connect(args.warning_db)
        warn_cur = warn_con.cursor()
        warn_cur.execute("CREATE TABLE IF NOT EXISTS warnings_raw (valid datetime, text char, processed tinyint)")
        warn_cur.execute("UPDATE warnings_raw SET processed=1")
        warn_con.commit()

        # Save the data
        print("\tLoading into local database...")

        query = "INSERT INTO warnings_raw (valid, text) VALUES (?,?)"
        for warning in warnings:

            # Find the valid time
            match = re.search(
                r"(?P<timestamp>[0-9]{6}T[0-9]{4}Z)",
                warning,
            )
            valid = dateparser.parse(match.group("timestamp"), yearfirst=True).strftime(std_fmt)

            # Save the row
            warn_cur.execute(query, [valid, warning])

        warn_con.commit()

        print("\tWarning archival complete...")
    elif fresh:
        handle_warnings = False
        print("No warning WFOs provided...skipping...")

    #######################
    # Radar File Queueing #
    #######################

    if args.radar_sites:
        handle_radar = True
        print("Query and prep for Level II files...")

        """
        What this does:

        - Get lists of days and sites to query S3
        - Query S3 and filter keys for the (compressed) level II files we want
        - Make a nice data structure for time-based lookup of keys to download during the sim (radar_table)
        """
        radar_search_dates = pd.date_range(
            start=arc_start_time.strftime("%Y-%m-%d"),
            end=arc_end_time.strftime("%Y-%m-%d"),
            freq="D"
        )
        nexrad_remote_files = []
        for date, radar_id in product(radar_search_dates, args.radar_sites):
            s3_search = s3.list_objects_v2(
                Bucket='noaa-nexrad-level2',
                Prefix=date.strftime(f"%Y/%m/%d/{radar_id}")
            )
            if 'Contents' not in s3_search:
                print(date.strftime(f"No files found for {radar_id} on %Y-%m-%d"))
                continue
            try:
                for obj in s3_search['Contents']:
                    key = obj['Key'] 
                    match = l2_datetime_pattern.search(key)
                    if match:
                        file_time = pd.Timestamp("{Y}-{m}-{d}T{H}:{M}:{S}".format(**match.groupdict()))
                        if arc_start_time <= file_time <= arc_end_time:
                            nexrad_remote_files.append((radar_id, file_time, key))
            except KeyError:
                # None found!
                continue
        nexrad_table = pd.from_records(
            nexrad_remote_files,
            columns=('site_id', 'scan_time', 'file_key')
        ).sort_value('scan_time')
    else:
        handle_radar = False
        print("No radar sites provided...skipping...")

    ##################
    # Run Simulation #
    ##################

    # Dump status summary
    print("\n+-----------------------------------------------+")
    print("| All setup steps complete, ready for sim start |")
    print("+-----------------------------------------------+\n")

    if args.lsr_wfos:
        print(f"LSR Count: {lsr_count}")

    if handle_warnings:
        print(f"Warning Count: {len(warnings)}")

    if handle_radar:
        print(f"Radar File Count: {len(radar_table)}")

    # User prompt for continuation
    input("\nPress enter to continue when ready")
    print()
    cur_start_time = input(
        "When does the current simulation of the case start?\n\t(enter nothing for now):"
    )

    # Set start time
    if not cur_start_time or cur_start_time[0] in ["n", "N"]:
        cur_start_time = datetime.now(tz=pytz.UTC)
    else:
        cur_start_time = dateparser.parse(cur_start_time)

    timings = {
        "arc_start_time": arc_start_time.strftime(std_fmt),
        "cur_start_time": cur_start_time.strftime(std_fmt),
        "speed_factor": speed_factor,
    } 

    # Trigger start on API
    r = requests.put(
        args.api_base + "/simulation/timings",
        data={
            "auth": admin_passcode,
            "simulation_running": 1,
            **timings
        }
    )

    # Fail out if error on simulation start
    if r.status_code >= 400:
        print(f"FATAL ERROR: API responded with code {r.status_code}...aborting simulation...")
        print(r.text)

    # Start the big loop.
    print("INITIATE SIMULATION LOOP!")
    running = True
    previous_update = datetime.now(tz=pytz.UTC) - timedelta(hours=1)  # only backfill at most 1 hour prior to now
    previous_api_check = datetime.now(tz=pytz.UTC) - timedelta(hours=1)
    while running:

        # Get the control timings for this loop iteration
        cur_now = now()
        arc_now = arc_time_from_cur(cur_now, timings)
        arc_previous = arc_time_from_cur(previous_update, timings)

        # Check occasionally if still running via API (i.e., allowed for delayed remote kill)
        if cur_now - previous_api_check > timedelta(seconds=60):
            if args.check_remote and not check_running(args.api_base):
                print("Simulation terminated on remote API...terminating here as well...")
                running = False
                break
            previous_api_check = cur_now

        # Check if we've based the simulation end time
        if arc_now > arc_end_time:
            print("Simulation end time reached...")

            r = requests.put(
                args.api_base + "/simulation/config",
                data={
                    "auth": admin_passcode,
                    "simulation_running": 1,
                }
            )

            # Fail out if error on simulation start
            if r.status_code >= 400:
                print(
                    f"FATAL ERROR: API responded with code {r.status_code}..."
                    "something just went wrong...manual intervention required to clean up..."
                )
                print(r.text)

            running=False
            break

        ###################
        # Warning release #
        ###################
        if handle_warnings:
            # Check if any warnings to release
            warn_cur.execute(
                "SELECT * FROM warnings_raw WHERE valid <= ? AND "
                + "(processed = 0 OR processed IS NULL)",
                [arc_now.strftime(std_fmt)],
            )
            warnings_to_release = warn_cur.fetchall()

            if len(warnings_to_release) > 0:

                print(
                    "Processing {} warning(s) for {}...".format(
                        len(warnings_to_release), cur_now.strftime(std_fmt)
                    )
                )

                # Loop over the warnings
                for warning_row in warnings_to_release:

                    # Release the warning!

                    # First, process the text
                    warning = warning_row[1]
                    warning, warning_cur_time = process_warning_text(
                        warning, timings=timings
                    )

                    # Then, determine the proper warning txt file, and append this
                    # warning
                    warning_file = args.warning_dir + warning_cur_time.strftime(
                        "warnings_%Y%m%d_%H.txt"
                    )
                    with open(warning_file, "a") as f:
                        f.write(warning)

                    # Finally, log it as processed
                    warn_cur.execute(
                        "UPDATE warnings_raw SET processed = 1 WHERE text = ?", [warning_row[1]]
                    )
                    warn_con.commit()

                print("\tWarnings processed.")

            else:
                print("No warnings to release for {}.".format(cur_now.strftime(std_fmt)))

        ###########################################
        # Radar scan download, munge, and release #
        ###########################################
        if handle_radar:
            scans_to_release = radar_table[(radar_table['arc_time'] <= arc_now) & (radar_table['arc_time'] > arc_previous)]
            proper_dir = os.path.dirname(os.path.realpath(__file__))
            os.chdir(args.radar_temp_dir)

            if len(scans_to_release) > 0:

                print(
                    "Processing {} scans for {}...".format(
                        len(scans_to_release), cur_now.strftime(std_fmt)
                    )
                )

                # Loop over the scans
                for _, scan_row in scans_to_release.iterrows():

                    # Release the scan!

                    # Get scan vars
                    site = scan_row['site_id']
                    arc_scan_time = scan_row['scan_time']
                    cur_scan_time = cur_time_from_arc(arc_scan_time, timings)
                    file_key = scan_row['file_key']
                    downloaded_l2 = file_key.split("/")[-1]

                    # Download in args.radar_temp_dir and extract to arc_file
                    print(f"\tDownloading {downloaded_l2}...")
                    s3.download_file("noaa-nexrad-level2", file_key, downloaded_l2)

                    if downloaded_l2[-3:] == 'V06':
                        # internally compressed
                        arc_file = downloaded_l2 + ".uncompressed"
                        debz(downloaded_l2, arc_file)
                    elif downloaded_l2[-6:] in ('V06.gz', 'V03.gz'):
                        # externally compressed
                        arc_file = downloaded_l2[:-3]
                        subprocess.run(["gunzip", "-v", downloaded_l2])
                    else:
                        print(f"File {downloaded_l2} not understood.")
                        continue

                    # Status
                    print("\t{}\t{} --> {}".format(site, arc_scan_time, cur_scan_time))

                    # Verify that the site directory exists (if not, create it)
                    site_deploy_dir = args.radar_dir + site + "/"
                    if not os.path.isdir(site_deploy_dir):
                        os.makedirs(site_deploy_dir)

                    # Munge and move
                    munge_cmd = proper_dir + "../munge/l2munger {} {} {} {}".format(
                        site,
                        dateparser.parse(cur_scan_time).strftime("%Y/%m/%d %H:%M:%S"),
                        config.speed_factor,
                        arc_file,
                    )
                    list_cmd = (
                        "ls -l {site}* | awk '{{print $5 \" \" $9}}' > " + "dir.list"
                    ).format(site=site)
                    cur_file = dateparser.parse(cur_scan_time).strftime(site + "%Y%m%d_%H%M%S")

                    os.system(munge_cmd)
                    shutil.copyfile(cur_file, site_deploy_dir + cur_file)
                    os.remove(cur_file)
                    os.chdir(site_deploy_dir)
                    os.system(list_cmd)
                    os.chdir(args.radar_temp_dir)

                print("\tScans processed.")

            else:
                print("No scans to release for {}.".format(cur_now.strftime(std_fmt)))

            os.chdir(proper_dir)

        # Control next iteration
        time_gap = int((cur_now + timedelta(seconds=default_sleep) - now()).total_seconds())
        if time_gap > 0:
            print(f"Waiting for {time_gap} seconds to next update")
            time.sleep(time_gap)
        else:
            print(f"Ooops, running behind by {-time_gap} seconds...continuing immediately...")
        
        previous_update = cur_now

    print("\nComplete!")
